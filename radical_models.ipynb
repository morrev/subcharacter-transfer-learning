{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decomposition_utils import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component embedding shape: (218, 200)\n",
      "UNK idx reserved for id: 218\n",
      "PAD idx reserved for id: 217\n"
     ]
    }
   ],
   "source": [
    "comp2vec_filepath = \"data/JWE-pretrained/radical_comp_vec\"\n",
    "char2comp_filepath = \"data/JWE/subcharacter/char2comp.txt\"\n",
    "\n",
    "comp_vocab_size, comp_embedding_size, comp2id, comp_embeddings, pad_idx, unk_idx = parse_comp2vec(comp2vec_filepath)\n",
    "char2id, comp_list = parse_char2comp(char2comp_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component vocab size:\t\t217\n",
      "Component embedding size:\t200\n",
      "Example components:\t\t{'儿': 0, '鹿': 1, '鹵': 2, '丶': 3, '車': 4}\n",
      "Component embeddings shape:\t(218, 200)\n",
      "UNK index:\t\t\t218\n",
      "PAD index:\t\t\t217\n"
     ]
    }
   ],
   "source": [
    "print(f\"Component vocab size:\\t\\t{comp_vocab_size}\")\n",
    "print(f\"Component embedding size:\\t{comp_embedding_size}\")\n",
    "print(f\"Example components:\\t\\t{dict(list(comp2id.items())[0:5])}\")\n",
    "print(f\"Component embeddings shape:\\t{comp_embeddings.shape}\")\n",
    "print(f\"UNK index:\\t\\t\\t{unk_idx}\")\n",
    "print(f\"PAD index:\\t\\t\\t{pad_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%cd data/GDCE-SSA/\n",
    "from src.util.load_data import load_data\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = load_data('data/GDCE-SSA/data/pickle/train_livedoor.pkl')['data']\n",
    "test_dataset = load_data('data/GDCE-SSA/data/pickle/test_livedoor.pkl')['data']\n",
    "X_train, y_train = list(zip(*train_dataset))\n",
    "X_test, y_test = list(zip(*test_dataset))\n",
    "\n",
    "# Create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train, \n",
    "                                                  test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum decomposition length: 41\n",
      "Maximum decomposition length: 36\n",
      "Maximum decomposition length: 35\n"
     ]
    }
   ],
   "source": [
    "# Convert to component IDs\n",
    "train_subcomponent_ids, max_decomposition_length = decompose(X_train, comp_list, comp2id, char2id, \n",
    "                                                              unk_idx, pad_idx, \n",
    "                                                              pad_length = None)\n",
    "val_subcomponent_ids, _ = decompose(X_val, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                     pad_length = max_decomposition_length)\n",
    "test_subcomponent_ids, _ = decompose(X_test, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                      pad_length = max_decomposition_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char-v2\")\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length = 512)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length = 512)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length = 512)\n",
    "\n",
    "# Initialize Dataset\n",
    "train_dataset = ComponentDataset(train_encodings, y_train, train_subcomponent_ids)\n",
    "val_dataset = ComponentDataset(val_encodings, y_val, val_subcomponent_ids)\n",
    "test_dataset = ComponentDataset(test_encodings, y_test, test_subcomponent_ids)\n",
    "\n",
    "# Initialize DataLoader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Sports Watch】格下相手に2試合連続スコアレスドローも、新システムは「やめる必要は全くない」\n",
      "12200\n",
      "['木', '夂', '口']\n",
      "130\n",
      "[218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 130, 33, 174, 218, 130, 202, 5, 218, 218, 218, 93, 30, 174, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 117, 130, 70, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 218, 52, 218, 147, 218, 218, 218, 218, 218]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(char2id['格'])\n",
    "print(comp_list[char2id['格']])\n",
    "print(comp2id[comp_list[char2id['格']][0]])\n",
    "print(text2subcomponent(X_train[0], comp_list, comp2id, char2id, unk_idx = unk_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
