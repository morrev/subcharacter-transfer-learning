{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposition_utils: character decomposition util functions\n",
    "# models defines: CustomBert, train_loop, test_loop\n",
    "from decomposition_utils import *\n",
    "from models import *\n",
    "from data_utils import load_livedoor, write_pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, BertModel, BertConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-japanese-livedoor-glyph\n",
      "Trained model does not exist!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N_EPOCHS = 1\n",
    "LR = 1e-5\n",
    "PATIENCE = 2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Type of models\n",
    "pooled = 1  # if 1, pooled; if 0, unpooled \n",
    "subcomponent = 2 # if 0, radical; if 1, subcomponent; if 2, glyph; if 3 or other number, baseline\n",
    "frozen = 0 # if 1, frozen weights; if 0, unfrozen\n",
    "livedoor = 1 # if 1, load livedoor data; if 0, load wikipedia data\n",
    "\n",
    "# Fefine filename to load from saved model\n",
    "if pooled: \n",
    "    fname = 'bert-base-japanese'\n",
    "    if livedoor:\n",
    "        fname += '-livedoor'\n",
    "        n_labels = 9 # number of classification labels\n",
    "    else: \n",
    "        fname += '-wikipedia'\n",
    "        n_labels = 12\n",
    "        \n",
    "    if subcomponent == 0: \n",
    "        fname += '-JWE-radical'\n",
    "    elif subcomponent == 1: \n",
    "        fname += '-JWE-subcomponent'\n",
    "    elif subcomponent == 2: \n",
    "        fname += '-glyph'\n",
    "        \n",
    "    if frozen: \n",
    "        fname += '-frozen'\n",
    "else: \n",
    "    fname = 'unpooled_jbert_cbert'\n",
    "    \n",
    "    if subcomponent == 0: \n",
    "        fname += '_radical'\n",
    "    elif subcomponent == 1: \n",
    "        fname += '_subcomponent'\n",
    "    elif subcomponent == 2: \n",
    "        fname += '_glyph'\n",
    "    \n",
    "    if livedoor: \n",
    "        fname += '_livedoor.pk'\n",
    "        n_labels = 9\n",
    "    else:\n",
    "        fname += '_wiki.pk'\n",
    "        n_labels = 9\n",
    "\n",
    "print(fname)\n",
    "\n",
    "# check if file exists \n",
    "file_exists = os.path.exists(os.getcwd() + \"/data/models/\" + fname)\n",
    "\n",
    "if file_exists: \n",
    "    print('Trained model already exists!')\n",
    "else: \n",
    "    print('Trained model does not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subcomponent / radical mapping definition & load JWE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subcomponent != 2:\n",
    "    if subcomponent == 1: \n",
    "        comp2vec_filepath = os.getcwd() + \"/data/JWE-pretrained/subcomponent_comp_vec\"\n",
    "        char2comp_filepath = os.getcwd() + \"/data/JWE/subcharacter/char2comp.txt\"\n",
    "    elif subcomponent == 0:\n",
    "        comp2vec_filepath = os.getcwd() + \"/data/JWE-pretrained/radical_comp_vec\"\n",
    "        char2comp_filepath = os.getcwd() + \"/data/JWE/subcharacter/char2radical.txt\"\n",
    "\n",
    "    comp_vocab_size, comp_embedding_size, comp2id, comp_embeddings, pad_idx, unk_idx = parse_comp2vec(comp2vec_filepath)\n",
    "    char2id, comp_list = parse_char2comp(char2comp_filepath)\n",
    "\n",
    "    # add UNK embeddings\n",
    "    if not pooled: \n",
    "        sub_embs_size = comp_embeddings.shape[-1]\n",
    "        unk_sub_emb = np.full(sub_embs_size, 0).reshape(1,-1)\n",
    "        SUBCOMPONENT_EMBEDDINGS_EXT = np.concatenate([comp_embeddings, unk_sub_emb], axis=0)\n",
    "\n",
    "    print(f\"Component vocab size:\\t\\t{comp_vocab_size}\")\n",
    "    print(f\"Component embedding size:\\t{comp_embedding_size}\")\n",
    "    print(f\"Example components:\\t\\t{dict(list(comp2id.items())[0:5])}\")\n",
    "    print(f\"Component embeddings shape:\\t{comp_embeddings.shape}\")\n",
    "    print(f\"UNK index:\\t\\t\\t{unk_idx}\")\n",
    "    print(f\"PAD index:\\t\\t\\t{pad_idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models.modeling_glycebert'; 'models' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4be5bb34da5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msubcomponent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/zoe/Desktop/CS287/subcharacter-transfer-learning/data/CBERT/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_glycebert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlyceBertModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models.modeling_glycebert'; 'models' is not a package"
     ]
    }
   ],
   "source": [
    "if subcomponent == 2:\n",
    "    os.chdir('/Users/zoe/Desktop/CS287/subcharacter-transfer-learning/data/ChineseBert/')\n",
    "    from models.modeling_glycebert import GlyceBertModel\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = load_livedoor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing \n",
    "iend = 50\n",
    "X_train = X_train[:iend]\n",
    "X_val = X_val[:iend]\n",
    "X_test = X_test[:iend]\n",
    "y_train = y_train[:iend]\n",
    "y_val = y_val[:iend]\n",
    "y_test = y_test[:iend]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data tokenization and DataLoader definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum decomposition length: 47\n",
      "Maximum decomposition length: 40\n",
      "Maximum decomposition length: 46\n"
     ]
    }
   ],
   "source": [
    "# Pooled model tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char-v2\")\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length = 512)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length = 512)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length = 512)\n",
    "\n",
    "# Convert to component IDs\n",
    "if pooled:\n",
    "    train_subcomponent_ids, max_decomposition_length = decompose(X_train, comp_list, comp2id, char2id, \n",
    "                                                                  unk_idx, pad_idx, \n",
    "                                                                  pad_length = None)\n",
    "    val_subcomponent_ids, _ = decompose(X_val, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                         pad_length = max_decomposition_length)\n",
    "    test_subcomponent_ids, _ = decompose(X_test, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                          pad_length = max_decomposition_length)\n",
    "    \n",
    "    train_seq_lengths=None\n",
    "    val_seq_lengths=None\n",
    "    test_seq_lengths=None\n",
    "\n",
    "else:\n",
    "    train_subcomponent_ids = [text2subcomponent(i, comp_list, comp2id, char2id, unk_idx, pooled, \n",
    "                                                tokenizer=tokenizer) for i in X_train]\n",
    "    val_subcomponent_ids = [text2subcomponent(i, comp_list, comp2id, char2id, unk_idx, pooled,\n",
    "                                              tokenizer=tokenizer) for i in X_val]\n",
    "    test_subcomponent_ids = [text2subcomponent(i, comp_list, comp2id, char2id, unk_idx, pooled,\n",
    "                                               tokenizer=tokenizer) for i in X_test]\n",
    "\n",
    "    # sequence lengths for unpooled models\n",
    "    train_seq_lengths = [len(ids) for ids in train_subcomponent_ids]\n",
    "    val_seq_lengths = [len(ids) for ids in val_subcomponent_ids]\n",
    "    test_seq_lengths = [len(ids) for ids in test_subcomponent_ids]\n",
    "    \n",
    "    TRAIN_SEQ_LEN = len(train_encodings['input_ids'][0])\n",
    "    VAL_SEQ_LEN = len(val_encodings['input_ids'][0])\n",
    "    TEST_SEQ_LEN = len(test_encodings['input_ids'][0])\n",
    "\n",
    "    train_subcomponent_ids = subcomponent2emb(train_subcomponent_ids, SUBCOMPONENT_EMBEDDINGS_EXT, padding=True, seq_length = TRAIN_SEQ_LEN)\n",
    "    val_subcomponent_ids = subcomponent2emb(val_subcomponent_ids, SUBCOMPONENT_EMBEDDINGS_EXT, padding=True, seq_length = VAL_SEQ_LEN)\n",
    "    test_subcomponent_ids = subcomponent2emb(test_subcomponent_ids, SUBCOMPONENT_EMBEDDINGS_EXT, padding=True, seq_length = TEST_SEQ_LEN)\n",
    "\n",
    "\n",
    "# Initialize Dataset\n",
    "train_dataset = ComponentDataset(train_encodings, y_train, train_subcomponent_ids, pooled, train_seq_lengths)\n",
    "val_dataset = ComponentDataset(val_encodings, y_val, val_subcomponent_ids, pooled, val_seq_lengths)\n",
    "test_dataset = ComponentDataset(test_encodings, y_test, test_subcomponent_ids, pooled, test_seq_lengths)\n",
    "\n",
    "# Initialize DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(char2id['格'])\n",
    "print(comp_list[char2id['格']])\n",
    "print(comp2id[comp_list[char2id['格']][0]])\n",
    "print(text2subcomponent(X_train[0], comp_list, comp2id, char2id, unk_idx, pooled, tokenizer=tokenizer)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Garbage collect\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "if file_exists: \n",
    "    if pooled: \n",
    "        bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-char-v2')\n",
    "        model = CustomPooledModel(bert, \n",
    "                                  embeddings = comp_embeddings,\n",
    "                                  num_labels = n_labels, \n",
    "                                  component_pad_idx = pad_idx).to(device)\n",
    "        model.load_state_dict(torch.load(os.getcwd() + \"/data/models/\" + fname +'/pytorch_model.bin', \n",
    "                                         map_location=torch.device(device)  ))\n",
    "    else: \n",
    "        JAPBERT_EMB_SIZE = 768\n",
    "        LSTM_INPUT_SIZE = JAPBERT_EMB_SIZE + comp_embedding_size\n",
    "        hidden_size = 200\n",
    "        model = LSTMClassifier(lstm_input_size=LSTM_INPUT_SIZE, \n",
    "                               hidden_size = hidden_size, \n",
    "                               output_size = n_labels,\n",
    "                               padding_idx = pad_idx, \n",
    "                               bertconfig = 'cl-tohoku/bert-base-japanese-char-v2')\n",
    "        model.load_state_dict(torch.load(os.getcwd() + \"/data/models/\" + fname, \n",
    "                                         map_location=torch.device(device)  ))\n",
    "    \n",
    "else: \n",
    "    \n",
    "    if pooled: \n",
    "        \n",
    "        # BertModel: from transformer docs:\n",
    "        # \"bare Bert Model transformer outputting raw hidden-states without any specific head on top\"\n",
    "        bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-char-v2')\n",
    "        model = CustomPooledModel(bert, \n",
    "                                 embeddings = comp_embeddings,\n",
    "                                 num_labels = n_labels, \n",
    "                                 component_pad_idx = pad_idx).to(device)\n",
    "    else: \n",
    "        JAPBERT_EMB_SIZE = 768\n",
    "        LSTM_INPUT_SIZE = JAPBERT_EMB_SIZE + comp_embedding_size\n",
    "        hidden_size = 200\n",
    "        model = LSTMClassifier(lstm_input_size=LSTM_INPUT_SIZE, \n",
    "                               hidden_size = hidden_size, \n",
    "                               output_size = n_labels,\n",
    "                               padding_idx = pad_idx, \n",
    "                               bertconfig = 'cl-tohoku/bert-base-japanese-char-v2')\n",
    "\n",
    "    # Freeze component embedding weights\n",
    "    if frozen: \n",
    "        for param in model.subcomponent_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = LR)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, 'min', patience = PATIENCE, verbose = True)\n",
    "\n",
    "    train_losses = []; train_accuracies = []\n",
    "    test_losses = []; test_accuracies = []\n",
    "\n",
    "    for e in range(N_EPOCHS):\n",
    "        print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "        train_loss, train_acc = train_loop(train_loader, model, optimizer, device, pooled=pooled)\n",
    "        test_loss, test_acc = test_loop(val_loader, model, lr_scheduler, device, pooled=pooled)\n",
    "        lr_scheduler.step(test_loss)\n",
    "        train_losses.append(train_loss); train_accuracies.append(train_acc)\n",
    "        test_losses.append(test_loss); test_accuracies.append(test_acc)\n",
    "    \n",
    "    # save model\n",
    "    write_pickle(os.getcwd() + \"/data/models/\" + fname, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# evaluate on test dataset\n",
    "test_predictions = []; test_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        component_ids = batch['subcomponent_ids'].to(device)\n",
    "        if not pooled: \n",
    "            lens_ = batch['seq_lengths']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, lens=lens_, \n",
    "                            labels=labels,comp_embeddings = component_ids)\n",
    "        else:\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                            labels=labels, subcomponent_ids = component_ids, device=device)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        test_predictions.extend(predictions)\n",
    "        test_labels.extend(labels.cpu())\n",
    "\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_labels = np.array(test_labels)\n",
    "print(f\"Test accuracy: {np.mean(test_predictions == test_labels)}\")\n",
    "\n",
    "# classification metrics on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
