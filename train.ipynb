{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposition_utils: character decomposition util functions\n",
    "# models defines: CustomBert, train_loop, test_loop\n",
    "from decomposition_utils import *\n",
    "from models import *\n",
    "from data_utils import load_livedoor, write_pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, BertModel, BertConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpooled_jbert_cbert_subcomponent_livedoor.pk\n",
      "Trained model does not exist!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N_EPOCHS = 5\n",
    "LR = 1e-5\n",
    "PATIENCE = 2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Type of models\n",
    "pooled = 0  # if 1, pooled; if 0, unpooled \n",
    "subcomponent = 1 # if 0, radical; if 1, subcomponent; if 2, glyph; if 3 or other number, baseline\n",
    "frozen = 0 # if 1, frozen weights; if 0, unfrozen\n",
    "livedoor = 1 # if 1, load livedoor data; if 0, load wikipedia data\n",
    "\n",
    "# define filename to save load from saved model\n",
    "if pooled: \n",
    "    fname = 'bert-base-japanese'\n",
    "    if livedoor:\n",
    "        fname += '-livedoor'\n",
    "        n_labels = 9 # number of classification labels\n",
    "    else: \n",
    "        fname += '-wikipedia'\n",
    "        n_labels = 12\n",
    "        \n",
    "    if subcomponent == 0: \n",
    "        fname += '-JWE-radical'\n",
    "    elif subcomponent == 1: \n",
    "        fname += '-JWE-subcomponent'\n",
    "    elif subcomponent == 2: \n",
    "        fname += '-glyph'\n",
    "        \n",
    "    if frozen: \n",
    "        fname += '-frozen'\n",
    "else: \n",
    "    fname = 'unpooled_jbert_cbert'\n",
    "    \n",
    "    if subcomponent == 0: \n",
    "        fname += '_radical'\n",
    "    elif subcomponent == 1: \n",
    "        fname += '_subcomponent'\n",
    "    elif subcomponent == 2: \n",
    "        fname += '_glyph'\n",
    "    \n",
    "    if livedoor: \n",
    "        fname += '_livedoor.pk'\n",
    "        n_labels = 9\n",
    "    else:\n",
    "        fname += '_wiki.pk'\n",
    "        n_labels = 9\n",
    "\n",
    "print(fname)\n",
    "\n",
    "# check if file exists \n",
    "file_exists = os.path.exists(os.getcwd() + \"/data/models/\" + fname)\n",
    "\n",
    "if file_exists: \n",
    "    print('Trained model already exists!')\n",
    "else: \n",
    "    print('Trained model does not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subcomponent / radical mapping definition & load JWE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component embedding shape: (13253, 200)\n",
      "UNK idx reserved for id: 13253\n",
      "PAD idx reserved for id: 13252\n"
     ]
    }
   ],
   "source": [
    "if subcomponent: \n",
    "    comp2vec_filepath = os.getcwd() + \"/data/JWE-pretrained/subcomponent_comp_vec\"\n",
    "    char2comp_filepath = os.getcwd() + \"/data/JWE/subcharacter/char2comp.txt\"\n",
    "else:\n",
    "    comp2vec_filepath = os.getcwd() + \"/data/JWE-pretrained/radical_comp_vec\"\n",
    "    char2comp_filepath = os.getcwd() + \"/data/JWE/subcharacter/char2radical.txt\"\n",
    "\n",
    "comp_vocab_size, comp_embedding_size, comp2id, comp_embeddings, pad_idx, unk_idx = parse_comp2vec(comp2vec_filepath)\n",
    "char2id, comp_list = parse_char2comp(char2comp_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component vocab size:\t\t13252\n",
      "Component embedding size:\t200\n",
      "Example components:\t\t{'遠': 0, '緂': 1, '糂': 2, '乔': 3, '籏': 4}\n",
      "Component embeddings shape:\t(13253, 200)\n",
      "UNK index:\t\t\t13253\n",
      "PAD index:\t\t\t13252\n"
     ]
    }
   ],
   "source": [
    "print(f\"Component vocab size:\\t\\t{comp_vocab_size}\")\n",
    "print(f\"Component embedding size:\\t{comp_embedding_size}\")\n",
    "print(f\"Example components:\\t\\t{dict(list(comp2id.items())[0:5])}\")\n",
    "print(f\"Component embeddings shape:\\t{comp_embeddings.shape}\")\n",
    "print(f\"UNK index:\\t\\t\\t{unk_idx}\")\n",
    "print(f\"PAD index:\\t\\t\\t{pad_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = load_livedoor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing \n",
    "iend = 10 \n",
    "X_train = X_train[:iend]\n",
    "X_val = X_val[:iend]\n",
    "X_test = X_test[:iend]\n",
    "y_train = y_train[:iend]\n",
    "y_val = y_val[:iend]\n",
    "y_test = y_test[:iend]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data tokenization and DataLoader definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum decomposition length: 18\n",
      "Maximum decomposition length: 27\n",
      "Maximum decomposition length: 29\n"
     ]
    }
   ],
   "source": [
    "# Pooled model tokenizer \n",
    "\n",
    "# Convert to component IDs\n",
    "train_subcomponent_ids, max_decomposition_length = decompose(X_train, comp_list, comp2id, char2id, \n",
    "                                                              unk_idx, pad_idx, \n",
    "                                                              pad_length = None)\n",
    "val_subcomponent_ids, _ = decompose(X_val, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                     pad_length = max_decomposition_length)\n",
    "test_subcomponent_ids, _ = decompose(X_test, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                      pad_length = max_decomposition_length)\n",
    "\n",
    "# sequence lengths for unpooled models\n",
    "train_seq_lengths = [len(ids) for ids in train_subcomponent_ids]\n",
    "val_seq_lengths = [len(ids) for ids in val_subcomponent_ids]\n",
    "test_seq_lengths = [len(ids) for ids in test_subcomponent_ids]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char-v2\")\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length = 512)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length = 512)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length = 512)\n",
    "\n",
    "# Initialize Dataset\n",
    "train_dataset = ComponentDataset(train_encodings, y_train, train_subcomponent_ids, train_seq_lengths)\n",
    "val_dataset = ComponentDataset(val_encodings, y_val, val_subcomponent_ids, val_seq_lengths)\n",
    "test_dataset = ComponentDataset(test_encodings, y_test, test_subcomponent_ids, test_seq_lengths)\n",
    "\n",
    "# Initialize DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# Unpooled model toeknizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Sports Watch】ダルビッシュ、ベンチ裏説教報道を否定\n",
      "12200\n",
      "['木', '夂', '口']\n",
      "210\n",
      "[13253, 13253, 13253, 13253, 13253, 13253, 13253, 13253, 13253, 13253]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(char2id['格'])\n",
    "print(comp_list[char2id['格']])\n",
    "print(comp2id[comp_list[char2id['格']][0]])\n",
    "print(text2subcomponent(X_train[0], comp_list, comp2id, char2id, unk_idx = unk_idx)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Garbage collect\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-char-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "torch.Size([2, 49, 768]) torch.Size([2, 18])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d6de92647693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {e+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpooled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpooled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS287/subcharacter-transfer-learning/models.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, optimizer, device, pooled)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpooled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlens_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq_lengths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             outputs = model(input_ids, attention_mask=attention_mask, lens=lens_, \n\u001b[0m\u001b[1;32m    128\u001b[0m                             comp_embeddings = component_ids)\n\u001b[1;32m    129\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS287/subcharacter-transfer-learning/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, lens, comp_embeddings)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0munpooled_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpooled_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mcombined_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munpooled_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_embeddings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m#LSTM architecture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "if file_exists: \n",
    "    \n",
    "    config = BertConfig.from_pretrained(os.getcwd() + \"/data/models/\" + fname + \"/config.json\")\n",
    "    model = CustomPooledModel.from_pretrained(os.getcwd() + \"/data/models/\" + fname, config=config).to(device)\n",
    "\n",
    "else: \n",
    "    \n",
    "    if pooled: \n",
    "        \n",
    "        # BertModel: from transformer docs:\n",
    "        # \"bare Bert Model transformer outputting raw hidden-states without any specific head on top\"\n",
    "        bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-char-v2')\n",
    "        model = CustomPooledModel(bert, \n",
    "                                 embeddings = comp_embeddings,\n",
    "                                 num_labels = n_labels, \n",
    "                                 component_pad_idx = pad_idx).to(device)\n",
    "    else: \n",
    "        JAPBERT_EMB_SIZE = 768\n",
    "        LSTM_INPUT_SIZE = JAPBERT_EMB_SIZE + comp_embedding_size\n",
    "        hidden_size = 200\n",
    "        model = CustomUnpooledModel(lstm_input_size=LSTM_INPUT_SIZE, \n",
    "                                    hidden_size = hidden_size, \n",
    "                                    output_size = n_labels,\n",
    "                                    padding_idx = pad_idx, \n",
    "                                    bertconfig = 'cl-tohoku/bert-base-japanese-char-v2')\n",
    "\n",
    "    # Freeze component embedding weights\n",
    "    if frozen: \n",
    "        for param in model.subcomponent_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = LR)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, 'min', patience = PATIENCE, verbose = True)\n",
    "\n",
    "    train_losses = []; train_accuracies = []\n",
    "    test_losses = []; test_accuracies = []\n",
    "\n",
    "    for e in range(N_EPOCHS):\n",
    "        print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "        train_loss, train_acc = train_loop(train_loader, model, optimizer, device, pooled=pooled)\n",
    "        test_loss, test_acc = test_loop(val_loader, model, lr_scheduler, device, pooled=pooled)\n",
    "        lr_scheduler.step(test_loss)\n",
    "        train_losses.append(train_loss); train_accuracies.append(train_acc)\n",
    "        test_losses.append(test_loss); test_accuracies.append(test_acc)\n",
    "    \n",
    "    # save model\n",
    "    model.write_pickle(os.getcwd() + \"/data/models/\" + fname, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
