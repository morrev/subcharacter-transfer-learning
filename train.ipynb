{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposition_utils: character decomposition util functions\n",
    "# models defines: CustomBert, train_loop, test_loop\n",
    "from decomposition_utils import *\n",
    "from models import *\n",
    "from data_utils import load_livedoor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N_EPOCHS = 1\n",
    "LR = 1e-5\n",
    "PATIENCE = 2\n",
    "BATCH_SIZE = 1\n",
    "pooled = 1  # if 1, pooled; if 0, unpooled \n",
    "subcomponent = 1 \n",
    "frozen = 1 # if 1, frozen weights; if 0, unfrozen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subcomponent definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component embedding shape: (218, 200)\n",
      "UNK idx reserved for id: 218\n",
      "PAD idx reserved for id: 217\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/zoe/Desktop/CS287/subcharacter-transfer-learning/data/JWE/subcharacter/char2comp.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-41951fe0d899>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcomp_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_embedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_comp2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp2vec_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mchar2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_char2comp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2comp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/CS287/subcharacter-transfer-learning/decomposition_utils.py\u001b[0m in \u001b[0;36mparse_char2comp\u001b[0;34m(char2comp_filepath)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# create Chinese character to index mapping dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# and list of all subcomponent decompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2comp_filepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/zoe/Desktop/CS287/subcharacter-transfer-learning/data/JWE/subcharacter/char2comp.txt'"
     ]
    }
   ],
   "source": [
    "comp2vec_filepath = os.getcwd() + \"/data/JWE-pretrained/radical_comp_vec\"\n",
    "char2comp_filepath = os.getcwd() + \"/data/JWE/subcharacter/char2comp.txt\"\n",
    "\n",
    "comp_vocab_size, comp_embedding_size, comp2id, comp_embeddings, pad_idx, unk_idx = parse_comp2vec(comp2vec_filepath)\n",
    "char2id, comp_list = parse_char2comp(char2comp_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component vocab size:\t\t217\n",
      "Component embedding size:\t200\n",
      "Example components:\t\t{'儿': 0, '鹿': 1, '鹵': 2, '丶': 3, '車': 4}\n",
      "Component embeddings shape:\t(218, 200)\n",
      "UNK index:\t\t\t218\n",
      "PAD index:\t\t\t217\n"
     ]
    }
   ],
   "source": [
    "print(f\"Component vocab size:\\t\\t{comp_vocab_size}\")\n",
    "print(f\"Component embedding size:\\t{comp_embedding_size}\")\n",
    "print(f\"Example components:\\t\\t{dict(list(comp2id.items())[0:5])}\")\n",
    "print(f\"Component embeddings shape:\\t{comp_embeddings.shape}\")\n",
    "print(f\"UNK index:\\t\\t\\t{unk_idx}\")\n",
    "print(f\"PAD index:\\t\\t\\t{pad_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = load_livedoor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data tokenization and DataLoader definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum decomposition length: 41\n",
      "Maximum decomposition length: 36\n",
      "Maximum decomposition length: 35\n"
     ]
    }
   ],
   "source": [
    "# Convert to component IDs\n",
    "train_subcomponent_ids, max_decomposition_length = decompose(X_train, comp_list, comp2id, char2id, \n",
    "                                                              unk_idx, pad_idx, \n",
    "                                                              pad_length = None)\n",
    "val_subcomponent_ids, _ = decompose(X_val, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                     pad_length = max_decomposition_length)\n",
    "test_subcomponent_ids, _ = decompose(X_test, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                      pad_length = max_decomposition_length)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char-v2\")\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length = 512)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length = 512)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length = 512)\n",
    "\n",
    "# Initialize Dataset\n",
    "train_dataset = ComponentDataset(train_encodings, y_train, train_subcomponent_ids)\n",
    "val_dataset = ComponentDataset(val_encodings, y_val, val_subcomponent_ids)\n",
    "test_dataset = ComponentDataset(test_encodings, y_test, test_subcomponent_ids)\n",
    "\n",
    "# Initialize DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Sports Watch】格下相手に2試合連続スコアレスドローも、新システムは「やめる必要は全くない」\n",
      "12200\n",
      "['木', '夂', '口']\n",
      "130\n",
      "[218, 218, 218, 218, 218, 218, 218, 218, 218, 218]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(char2id['格'])\n",
    "print(comp_list[char2id['格']])\n",
    "print(comp2id[comp_list[char2id['格']][0]])\n",
    "print(text2subcomponent(X_train[0], comp_list, comp2id, char2id, unk_idx = unk_idx)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-char-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Garbage collect\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# BertModel: from transformer docs:\n",
    "# \"bare Bert Model transformer outputting raw hidden-states without any specific head on top\"\n",
    "bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-char-v2')\n",
    "model = CustomBert(bert, \n",
    "                   embeddings = comp_embeddings,\n",
    "                   num_labels = 9, \n",
    "                   component_pad_idx = pad_idx).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze component embedding weights\n",
    "for param in model.subcomponent_embedding.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = LR)\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, 'min', patience = PATIENCE, verbose = True)\n",
    "\n",
    "train_losses = []; train_accuracies = []\n",
    "test_losses = []; test_accuracies = []\n",
    "\n",
    "for e in range(N_EPOCHS):\n",
    "    print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "    train_loss, train_acc = train_loop(train_loader, model, optimizer, device)\n",
    "    test_loss, test_acc = test_loop(val_loader, model, lr_scheduler, device)\n",
    "    lr_scheduler.step(test_loss)\n",
    "    train_losses.append(train_loss); train_accuracies.append(train_acc)\n",
    "    test_losses.append(test_loss); test_accuracies.append(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
