{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposition_utils: character decomposition util functions\n",
    "# models defines: CustomBert, train_loop, test_loop\n",
    "from decomposition_utils import *\n",
    "from models import *\n",
    "from data_utils import load_livedoor, load_wikipedia, write_pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, BertModel, BertConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-japanese-livedoor-glyph\n",
      "Trained model does not exist!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N_EPOCHS = 1\n",
    "LR = 1e-5\n",
    "PATIENCE = 2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Type of models\n",
    "pooled = 1  # if 1, pooled; if 0, unpooled \n",
    "subcomponent = 2 # if 0, radical; if 1, subcomponent; if 2, glyph; if 3 or other number, baseline\n",
    "frozen = 0 # if 1, frozen weights; if 0, unfrozen\n",
    "livedoor = 1 # if 1, load livedoor data; if 0, load wikipedia data\n",
    "\n",
    "# Define filename to load from saved model\n",
    "if pooled: \n",
    "    fname = 'bert-base-japanese'\n",
    "    if livedoor:\n",
    "        fname += '-livedoor'\n",
    "        n_labels = 9 # number of classification labels\n",
    "    else: \n",
    "        fname += '-wikipedia'\n",
    "        n_labels = 12\n",
    "        \n",
    "    if subcomponent == 0: \n",
    "        fname += '-JWE-radical'\n",
    "    elif subcomponent == 1: \n",
    "        fname += '-JWE-subcomponent'\n",
    "    elif subcomponent == 2: \n",
    "        fname += '-glyph'\n",
    "        \n",
    "    if frozen: \n",
    "        fname += '-frozen'\n",
    "else: \n",
    "    fname = 'unpooled_jbert_cbert'\n",
    "    \n",
    "    if subcomponent == 0: \n",
    "        fname += '_radical'\n",
    "    elif subcomponent == 1: \n",
    "        fname += '_subcomponent'\n",
    "    elif subcomponent == 2: \n",
    "        fname += '_glyph'\n",
    "    \n",
    "    if livedoor: \n",
    "        fname += '_livedoor.pk'\n",
    "        n_labels = 9\n",
    "    else:\n",
    "        fname += '_wiki.pk'\n",
    "        n_labels = 9\n",
    "\n",
    "print(fname)\n",
    "\n",
    "# check if file exists \n",
    "file_exists = os.path.exists(os.getcwd() + \"/data/models/\" + fname)\n",
    "\n",
    "if file_exists: \n",
    "    print('Trained model already exists!')\n",
    "else: \n",
    "    print('Trained model does not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subcomponent / radical mapping definition & load JWE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if subcomponent != 2:\n",
    "    if subcomponent == 1: \n",
    "        comp2vec_filepath = os.getcwd() + \"/data/JWE-pretrained/subcomponent_comp_vec\"\n",
    "        char2comp_filepath = os.getcwd() + \"/data/JWE/subcharacter/char2comp.txt\"\n",
    "    elif subcomponent == 0:\n",
    "        comp2vec_filepath = os.getcwd() + \"/data/JWE-pretrained/radical_comp_vec\"\n",
    "        char2comp_filepath = os.getcwd() + \"/data/JWE/subcharacter/char2radical.txt\"\n",
    "\n",
    "    comp_vocab_size, comp_embedding_size, comp2id, comp_embeddings, pad_idx, unk_idx = parse_comp2vec(comp2vec_filepath)\n",
    "    char2id, comp_list = parse_char2comp(char2comp_filepath)\n",
    "\n",
    "    # add UNK embeddings\n",
    "    if not pooled: \n",
    "        sub_embs_size = comp_embeddings.shape[-1]\n",
    "        unk_sub_emb = np.full(sub_embs_size, 0).reshape(1,-1)\n",
    "        SUBCOMPONENT_EMBEDDINGS_EXT = np.concatenate([comp_embeddings, unk_sub_emb], axis=0)\n",
    "\n",
    "    print(f\"Component vocab size:\\t\\t{comp_vocab_size}\")\n",
    "    print(f\"Component embedding size:\\t{comp_embedding_size}\")\n",
    "    print(f\"Example components:\\t\\t{dict(list(comp2id.items())[0:5])}\")\n",
    "    print(f\"Component embeddings shape:\\t{comp_embeddings.shape}\")\n",
    "    print(f\"UNK index:\\t\\t\\t{unk_idx}\")\n",
    "    print(f\"PAD index:\\t\\t\\t{pad_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Chinese BERT glyph embeddings & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/zoe/Desktop/CS287/subcharacter-transfer-learning/data/ChineseBERT-large were not used when initializing GlyceBertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing GlyceBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GlyceBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "masterdir = os.getcwd() \n",
    "if subcomponent == 2:\n",
    "    \n",
    "    os.chdir(masterdir +'/data/ChineseBert/')\n",
    "    from datasets.bert_dataset import BertDataset\n",
    "    os.chdir(masterdir +'/data/ChineseBert/models')\n",
    "    from modeling_glycebert import GlyceBertModel\n",
    "    os.chdir(masterdir)\n",
    "    \n",
    "    CBERT_PATH = masterdir +'/data/ChineseBERT-large'\n",
    "    \n",
    "    # ChineseBERT tokenizer \n",
    "    chinese_bert_tokenizer = BertDataset(CBERT_PATH)\n",
    "    chinese_bert = GlyceBertModel.from_pretrained(CBERT_PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if livedoor:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_livedoor()\n",
    "else:\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_wikipedia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing \n",
    "iend = 50\n",
    "X_train = X_train[:iend]\n",
    "X_val = X_val[:iend]\n",
    "X_test = X_test[:iend]\n",
    "y_train = y_train[:iend]\n",
    "y_val = y_val[:iend]\n",
    "y_test = y_test[:iend]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data tokenization and DataLoader definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pooled model tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char-v2\")\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length = 512)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length = 512)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length = 512)\n",
    "\n",
    "# Convert to component IDs\n",
    "\n",
    "if pooled: \n",
    "    if subcomponent != 2:\n",
    "        train_subcomponent_ids, max_decomposition_length = decompose(X_train, comp_list, comp2id, char2id, \n",
    "                                                                      unk_idx, pad_idx, \n",
    "                                                                      pad_length = None)\n",
    "        val_subcomponent_ids, _ = decompose(X_val, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                             pad_length = max_decomposition_length)\n",
    "        test_subcomponent_ids, _ = decompose(X_test, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                              pad_length = max_decomposition_length)\n",
    "    else: # glyph embeddings\n",
    "        train_subcomponent_ids = get_glyph_embeddings(X_train, chinese_bert, chinese_bert_tokenizer)\n",
    "        val_subcomponent_ids = get_glyph_embeddings(X_val, chinese_bert, chinese_bert_tokenizer)\n",
    "        test_subcomponent_ids = get_glyph_embeddings(X_test, chinese_bert, chinese_bert_tokenizer)\n",
    "        \n",
    "    train_seq_lengths=None\n",
    "    val_seq_lengths=None\n",
    "    test_seq_lengths=None\n",
    "\n",
    "else: # unpooled \n",
    "    if subcomponent != 2: # JWE\n",
    "        train_subcomponent_ids = [text2subcomponent(i, comp_list, comp2id, char2id, unk_idx, pooled, \n",
    "                                                    tokenizer=tokenizer) for i in X_train]\n",
    "        val_subcomponent_ids = [text2subcomponent(i, comp_list, comp2id, char2id, unk_idx, pooled,\n",
    "                                                  tokenizer=tokenizer) for i in X_val]\n",
    "        test_subcomponent_ids = [text2subcomponent(i, comp_list, comp2id, char2id, unk_idx, pooled,\n",
    "                                                   tokenizer=tokenizer) for i in X_test]\n",
    "\n",
    "        # sequence lengths for unpooled models\n",
    "        train_seq_lengths = [len(ids) for ids in train_subcomponent_ids]\n",
    "        val_seq_lengths = [len(ids) for ids in val_subcomponent_ids]\n",
    "        test_seq_lengths = [len(ids) for ids in test_subcomponent_ids]\n",
    "        \n",
    "        TRAIN_SEQ_LEN = len(train_encodings['input_ids'][0])\n",
    "        VAL_SEQ_LEN = len(val_encodings['input_ids'][0])\n",
    "        TEST_SEQ_LEN = len(test_encodings['input_ids'][0])\n",
    "        \n",
    "        train_subcomponent_ids = subcomponent2emb(train_subcomponent_ids, SUBCOMPONENT_EMBEDDINGS_EXT, padding=True, seq_length = TRAIN_SEQ_LEN)\n",
    "        val_subcomponent_ids = subcomponent2emb(val_subcomponent_ids, SUBCOMPONENT_EMBEDDINGS_EXT, padding=True, seq_length = VAL_SEQ_LEN)\n",
    "        test_subcomponent_ids = subcomponent2emb(test_subcomponent_ids, SUBCOMPONENT_EMBEDDINGS_EXT, padding=True, seq_length = TEST_SEQ_LEN)\n",
    "        \n",
    "    else: \n",
    "        train_seq_lengths, train_subcomponent_ids = text2glyph(X_train, chinese_bert, chinese_bert_tokenizer, tokenizer)\n",
    "        val_seq_lengths, val_subcomponent_ids = text2glyph(X_val, chinese_bert, chinese_bert_tokenizer, tokenizer)\n",
    "        test_seq_lengths, test_subcomponent_ids = text2glyph(X_test, chinese_bert, chinese_bert_tokenizer, tokenizer)\n",
    "        \n",
    "        TRAIN_SEQ_LEN = len(train_subcomponent_ids)\n",
    "        VAL_SEQ_LEN = len(val_subcomponent_ids)\n",
    "        TEST_SEQ_LEN = len(test_subcomponent_ids)\n",
    "\n",
    "        \n",
    "# Initialize Dataset\n",
    "train_dataset = ComponentDataset(train_encodings, y_train, train_subcomponent_ids, pooled, train_seq_lengths)\n",
    "val_dataset = ComponentDataset(val_encodings, y_val, val_subcomponent_ids, pooled, val_seq_lengths)\n",
    "test_dataset = ComponentDataset(test_encodings, y_test, test_subcomponent_ids, pooled, test_seq_lengths)\n",
    "\n",
    "# Initialize DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train[0])\n",
    "# print(char2id['格'])\n",
    "# print(comp_list[char2id['格']])\n",
    "# print(comp2id[comp_list[char2id['格']][0]])\n",
    "# print(text2subcomponent(X_train[0], comp_list, comp2id, char2id, unk_idx, pooled, tokenizer=tokenizer)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Garbage collect\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-char-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]/Users/zoe/Desktop/CS287/subcharacter-transfer-learning/decomposition_utils.py:234: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item['subcomponent_ids'] = torch.tensor(self.subcomponent_ids[idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:41<00:00,  1.66s/it]\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 14.0%, Avg loss: 2.220189 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:07<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 26.0%, Avg loss: 2.115010 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "if subcomponent == 2: \n",
    "    pad_idx = 0\n",
    "\n",
    "    comp_embeddings = chinese_bert.embeddings.glyph_embeddings \n",
    "    \n",
    "if file_exists: \n",
    "    if pooled: \n",
    "        bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-char-v2')\n",
    "        model = CustomPooledModel(bert, \n",
    "                                  embeddings = comp_embeddings,\n",
    "                                  num_labels = n_labels, \n",
    "                                  component_pad_idx = pad_idx, \n",
    "                                  subcomponent = subcomponent).to(device)\n",
    "        model.load_state_dict(torch.load(os.getcwd() + \"/data/models/\" + fname +'/pytorch_model.bin', \n",
    "                                         map_location=torch.device(device)  ))\n",
    "    else: \n",
    "        JAPBERT_EMB_SIZE = 768\n",
    "        if subcomponent == 2: \n",
    "            comp_embedding_size = train_subcomponent_ids.shape[-1]\n",
    "        LSTM_INPUT_SIZE = JAPBERT_EMB_SIZE + comp_embedding_size\n",
    "        hidden_size = 200\n",
    "        model = LSTMClassifier(lstm_input_size=LSTM_INPUT_SIZE, \n",
    "                               hidden_size = hidden_size, \n",
    "                               output_size = n_labels,\n",
    "                               padding_idx = pad_idx, \n",
    "                               bertconfig = 'cl-tohoku/bert-base-japanese-char-v2')\n",
    "        model.load_state_dict(torch.load(os.getcwd() + \"/data/models/\" + fname, \n",
    "                                         map_location=torch.device(device)  ))\n",
    "    \n",
    "else: \n",
    "    \n",
    "    if pooled: \n",
    "        \n",
    "        # BertModel: from transformer docs:\n",
    "        # \"bare Bert Model transformer outputting raw hidden-states without any specific head on top\"\n",
    "        bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-char-v2')\n",
    "        model = CustomPooledModel(bert, \n",
    "                                 embeddings = comp_embeddings,\n",
    "                                 num_labels = n_labels, \n",
    "                                 component_pad_idx = pad_idx, \n",
    "                                 subcomponent = subcomponent).to(device)\n",
    "    else: \n",
    "        JAPBERT_EMB_SIZE = 768\n",
    "        if subcomponent == 2: \n",
    "            comp_embedding_size = train_subcomponent_ids.shape[-1]\n",
    "        LSTM_INPUT_SIZE = JAPBERT_EMB_SIZE + comp_embedding_size\n",
    "        hidden_size = 200\n",
    "        model = LSTMClassifier(lstm_input_size=LSTM_INPUT_SIZE, \n",
    "                               hidden_size = hidden_size, \n",
    "                               output_size = n_labels,\n",
    "                               padding_idx = pad_idx, \n",
    "                               bertconfig = 'cl-tohoku/bert-base-japanese-char-v2')\n",
    "\n",
    "    # Freeze component embedding weights\n",
    "    if frozen: \n",
    "        for param in model.subcomponent_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = LR)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, 'min', patience = PATIENCE, verbose = True)\n",
    "\n",
    "    train_losses = []; train_accuracies = []\n",
    "    test_losses = []; test_accuracies = []\n",
    "\n",
    "    for e in range(N_EPOCHS):\n",
    "        print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "        train_loss, train_acc = train_loop(train_loader, model, optimizer, device, pooled=pooled)\n",
    "        test_loss, test_acc = test_loop(val_loader, model, lr_scheduler, device, pooled=pooled)\n",
    "        lr_scheduler.step(test_loss)\n",
    "        train_losses.append(train_loss); train_accuracies.append(train_acc)\n",
    "        test_losses.append(test_loss); test_accuracies.append(test_acc)\n",
    "    \n",
    "    # save model\n",
    "    if pooled:\n",
    "        write_pickle(os.getcwd() + \"/data/models/\" + fname + '.pk', model)\n",
    "    else: \n",
    "        write_pickle(os.getcwd() + \"/data/models/\" + fname, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:09<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.33      0.46         9\n",
      "           1       0.00      0.00      0.00         4\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00         3\n",
      "           5       0.00      0.00      0.00         3\n",
      "           6       0.00      0.00      0.00         8\n",
      "           7       0.11      1.00      0.20         5\n",
      "           8       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.16        50\n",
      "   macro avg       0.10      0.15      0.07        50\n",
      "weighted avg       0.15      0.16      0.10        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/zoe/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zoe/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/zoe/opt/anaconda3/envs/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# evaluate on test dataset\n",
    "test_predictions = []; test_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        component_ids = batch['subcomponent_ids'].to(device)\n",
    "        if not pooled: \n",
    "            lens_ = batch['seq_lengths']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, lens=lens_, \n",
    "                            labels=labels,comp_embeddings = component_ids)\n",
    "        else:\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, \n",
    "                            labels=labels, subcomponent_ids = component_ids, device=device)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        test_predictions.extend(predictions)\n",
    "        test_labels.extend(labels.cpu())\n",
    "\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_labels = np.array(test_labels)\n",
    "print(f\"Test accuracy: {np.mean(test_predictions == test_labels)}\")\n",
    "\n",
    "# classification metrics on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_labels, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
