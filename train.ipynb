{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposition_utils: character decomposition util functions\n",
    "# models defines: CustomBert, train_loop, test_loop\n",
    "from decomposition_utils import *\n",
    "from models import *\n",
    "from data_utils import load_livedoor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, BertModel, BertConfig\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-japanese-livedoor-JWE-subcomponent\n",
      "Trained model already exists!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "N_EPOCHS = 5\n",
    "LR = 1e-5\n",
    "PATIENCE = 2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Type of models\n",
    "pooled = 1  # if 1, pooled; if 0, unpooled \n",
    "subcomponent = 1 # if 0, radical; if 1, subcomponent; if 2, glyph; if 3 or other number, baseline\n",
    "frozen = 0 # if 1, frozen weights; if 0, unfrozen\n",
    "livedoor = 1 # if 1, load livedoor data; if 0, load wikipedia data\n",
    "\n",
    "# define filename to save load from saved model\n",
    "if pooled: \n",
    "    fname = 'bert-base-japanese'\n",
    "    if livedoor:\n",
    "        fname += '-livedoor'\n",
    "        n_labels = 9 # number of classification labels\n",
    "    else: \n",
    "        fname += '-wikipedia'\n",
    "        n_labels = 12\n",
    "        \n",
    "    if subcomponent == 0: \n",
    "        fname += '-JWE-radical'\n",
    "    elif subcomponent == 1: \n",
    "        fname += '-JWE-subcomponent'\n",
    "    elif subcomponent == 2: \n",
    "        fname += '-glyph'\n",
    "        \n",
    "    if frozen: \n",
    "        fname += '-frozen'\n",
    "\n",
    "print(fname)\n",
    "\n",
    "# check if file exists \n",
    "file_exists = os.path.exists(os.getcwd() + \"/data/models/\" + fname)\n",
    "\n",
    "if file_exists: \n",
    "    print('Trained model already exists!')\n",
    "else: \n",
    "    print('Trained model does not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subcomponent / radical mapping definition & load JWE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component embedding shape: (13253, 200)\n",
      "UNK idx reserved for id: 13253\n",
      "PAD idx reserved for id: 13252\n"
     ]
    }
   ],
   "source": [
    "if subcomponent: \n",
    "    comp2vec_filepath = os.getcwd() + \"/data/JWE-pretrained/subcomponent_comp_vec\"\n",
    "    char2comp_filepath = os.getcwd() + \"/data/JWE/subcharacter/char2comp.txt\"\n",
    "else:\n",
    "    comp2vec_filepath = os.getcwd() + \"/data/JWE-pretrained/radical_comp_vec\"\n",
    "    char2comp_filepath = os.getcwd() + \"/data/JWE/subcharacter/char2radical.txt\"\n",
    "\n",
    "comp_vocab_size, comp_embedding_size, comp2id, comp_embeddings, pad_idx, unk_idx = parse_comp2vec(comp2vec_filepath)\n",
    "char2id, comp_list = parse_char2comp(char2comp_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component vocab size:\t\t13252\n",
      "Component embedding size:\t200\n",
      "Example components:\t\t{'遠': 0, '緂': 1, '糂': 2, '乔': 3, '籏': 4}\n",
      "Component embeddings shape:\t(13253, 200)\n",
      "UNK index:\t\t\t13253\n",
      "PAD index:\t\t\t13252\n"
     ]
    }
   ],
   "source": [
    "print(f\"Component vocab size:\\t\\t{comp_vocab_size}\")\n",
    "print(f\"Component embedding size:\\t{comp_embedding_size}\")\n",
    "print(f\"Example components:\\t\\t{dict(list(comp2id.items())[0:5])}\")\n",
    "print(f\"Component embeddings shape:\\t{comp_embeddings.shape}\")\n",
    "print(f\"UNK index:\\t\\t\\t{unk_idx}\")\n",
    "print(f\"PAD index:\\t\\t\\t{pad_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load and split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = load_livedoor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing \n",
    "iend = 10 \n",
    "X_train = X_train[:iend]\n",
    "X_val = X_val[:iend]\n",
    "X_test = X_test[:iend]\n",
    "y_train = y_train[:iend]\n",
    "y_val = y_val[:iend]\n",
    "y_test = y_test[:iend]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data tokenization and DataLoader definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum decomposition length: 18\n",
      "Maximum decomposition length: 27\n",
      "Maximum decomposition length: 29\n"
     ]
    }
   ],
   "source": [
    "# Pooled model tokenizer \n",
    "\n",
    "# Convert to component IDs\n",
    "train_subcomponent_ids, max_decomposition_length = decompose(X_train, comp_list, comp2id, char2id, \n",
    "                                                              unk_idx, pad_idx, \n",
    "                                                              pad_length = None)\n",
    "val_subcomponent_ids, _ = decompose(X_val, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                     pad_length = max_decomposition_length)\n",
    "test_subcomponent_ids, _ = decompose(X_test, comp_list, comp2id, char2id, unk_idx, pad_idx, \n",
    "                                      pad_length = max_decomposition_length)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-char-v2\")\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length = 512)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length = 512)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length = 512)\n",
    "\n",
    "# Initialize Dataset\n",
    "train_dataset = ComponentDataset(train_encodings, y_train, train_subcomponent_ids)\n",
    "val_dataset = ComponentDataset(val_encodings, y_val, val_subcomponent_ids)\n",
    "test_dataset = ComponentDataset(test_encodings, y_test, test_subcomponent_ids)\n",
    "\n",
    "# Initialize DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "# Unpooled model toeknizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Sports Watch】ダルビッシュ、ベンチ裏説教報道を否定\n",
      "12200\n",
      "['木', '夂', '口']\n",
      "210\n",
      "[13253, 13253, 13253, 13253, 13253, 13253, 13253, 13253, 13253, 13253]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(char2id['格'])\n",
    "print(comp_list[char2id['格']])\n",
    "print(comp2id[comp_list[char2id['格']][0]])\n",
    "print(text2subcomponent(X_train[0], comp_list, comp2id, char2id, unk_idx = unk_idx)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Garbage collect\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "# BertModel: from transformer docs:\n",
    "# \"bare Bert Model transformer outputting raw hidden-states without any specific head on top\"\n",
    "bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-char-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'CustomPooledModel' has no attribute 'from_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-feaac1ae2f51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/data/models/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomPooledModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/data/models/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'CustomPooledModel' has no attribute 'from_pretrained'"
     ]
    }
   ],
   "source": [
    "if file_exists: \n",
    "    \n",
    "    config = BertConfig.from_pretrained(os.getcwd() + \"/data/models/\" + fname + \"/config.json\")\n",
    "    model = CustomPooledModel.from_pretrained(os.getcwd() + \"/data/models/\" + fname, config=config).to(device)\n",
    "\n",
    "else: \n",
    "    \n",
    "    model = CustomPooledModel(bert, \n",
    "                             embeddings = comp_embeddings,\n",
    "                             num_labels = n_labels, \n",
    "                             component_pad_idx = pad_idx).to(device)\n",
    "\n",
    "    # Freeze component embedding weights\n",
    "    if frozen: \n",
    "        for param in model.subcomponent_embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr = LR)\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, 'min', patience = PATIENCE, verbose = True)\n",
    "\n",
    "    train_losses = []; train_accuracies = []\n",
    "    test_losses = []; test_accuracies = []\n",
    "\n",
    "    for e in range(N_EPOCHS):\n",
    "        print(f\"Epoch {e+1}\\n-------------------------------\")\n",
    "        train_loss, train_acc = train_loop(train_loader, model, optimizer, device)\n",
    "        test_loss, test_acc = test_loop(val_loader, model, lr_scheduler, device)\n",
    "        lr_scheduler.step(test_loss)\n",
    "        train_losses.append(train_loss); train_accuracies.append(train_acc)\n",
    "        test_losses.append(test_loss); test_accuracies.append(test_acc)\n",
    "    \n",
    "    model.save_pretrained(os.getcwd() + \"/data/models/\" + fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "def write_pickle(path, d):\n",
    "    try:\n",
    "      with open(path,'wb') as f:\n",
    "          return pickle.dump(d, f, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    except:\n",
    "        print(f'Write pickle error on {f}')"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
